<configuration>

<property>
  <name>yarn.nodemanager.runtime.linux.sandbox-mode.local-dirs.permissions</name>
  <value>read,write,execute,delete</value>
</property>

<property>
  <name>yarn.nodemanager.default-container-executor.log-dirs.permissions</name>
  <value>777</value>
</property>


<property>
  <name>yarn.nodemanager.local-dirs</name>
  <value>/tmp</value>
</property>

<property>
  <name>spark.yarn.scheduler.heartbeat.interval-ms</name>
  <value>500</value>
  <description>The interval in ms in which the Spark application master heartbeats into the YARN ResourceManager</description>
</property>

<!-- FAIR Scheduling -->

<property>
  <name>yarn.resourcemanager.scheduler.class</name>
  <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
</property>

<property>
  <name>yarn.scheduler.fair.preemption</name>
  <value>true</value>
</property>

<property>
  <name>yarn.scheduler.fair.user-as-default-queue</name>
  <value>true</value>
</property>

<property>
  <name>yarn.scheduler.fair.allocation.file</name>
  <value>fair-scheduler.xml</value>
</property>

<!-- FAIR Scheduling -->

<property>
        <name>yarn.resourcemanager.hostname</name>
        <value>portal1</value>
</property>

<!-- Fix YARN nodemanager resources -->

<property>
        <name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
        <value>true</value>
</property>

<property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>1</value>
</property>

<property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>1500</value>
        <description>
        The minimum value for this property is 500 MB. The default value for the ApplicationMaster to run is 1024 MB. The minimum amount 
        of memory for any Spark executor to run is 1408 MB (1024MB for the executor + 384 MB minimum overhead memory). We set it to 1500MB
        to have some minimum extra room to be safe in any case.
        </description>
</property>

<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>1500</value>
</property>

<property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>500</value>
</property>

<!-- Fix YARN nodemanager resources -->

<!-- Activate Spark shuffle service in YARN for dynamic allocation -->

<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>spark_shuffle</value>
  <description>shuffle service that needs to be set for Map Reduce to run</description>
</property>

<property>
  <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
  <value>org.apache.spark.network.yarn.YarnShuffleService</value>
  <description>
  This class is defined in $SPARK_HOME/yarn/spark-3.0.1-yarn-shuffle.jar and should be copied to $HADOOP_HOME/share/hadoop/yarn/lib
  </description>
</property>

<property>
  <name>spark.yarn.shuffle.stopOnFailure</name>
  <value>true</value>
  <description>
  Whether to stop the NodeManager when there's a failure in the Spark Shuffle Service's initialization.
  This prevents application failures caused by running containers on NodeManagers where the Spark Shuffle Service is not running.
  </description>
</property>

</configuration>
